{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9470f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 0 — Imports & setup\n",
    "# =========================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "pl.enable_string_cache()\n",
    "\n",
    "# Opcional: para ver tablas grandes sin truncado\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_cols(20)\n",
    "pl.Config.set_fmt_str_lengths(120)\n",
    "\n",
    "class PreprocessError(RuntimeError):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 1 — Config\n",
    "# =========================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PreprocessConfig:\n",
    "    # Carpetas\n",
    "    root_dir: Path                        # datos_competicion/\n",
    "    algos_subdir: str = \"algoritmos\"\n",
    "    cache_dir: Path = Path(\"data/cache\")       # donde escribir parquets\n",
    "    \n",
    "    # Columnas esperadas (robusto a variaciones)\n",
    "    dt_candidates: tuple[str, ...] = (\"datetime\", \"date\", \"timestamp\", \"time\")\n",
    "    close_candidates: tuple[str, ...] = (\"close\", \"Close\", \"c\", \"price\", \"last\")\n",
    "    \n",
    "    # Parsing datetime (para algos)\n",
    "    dt_format_candidates: tuple[str, ...] = (\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M:%S%.f\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"%Y-%m-%dT%H:%M:%S%.f\",\n",
    "    )\n",
    "    \n",
    "    # Calidad / filtros\n",
    "    min_obs: int = 60                    # mínimo de días con dato\n",
    "    min_coverage: float = 0.70           # n_obs / (end-start+1)\n",
    "    constant_close_std_eps: float = 1e-8 # close_std muy pequeño => constante\n",
    "    max_abs_ret_clip: float = 0.50       # clip robusto de returns diarios (+/- 50%)\n",
    "    \n",
    "    # Features RL\n",
    "    feature_windows: tuple[int, ...] = (20, 60, 120)\n",
    "    annualization_factor: int = 252      # trading days\n",
    "    \n",
    "    # Output names\n",
    "    panel_name: str = \"algos_panel.parquet\"\n",
    "    meta_name: str = \"algos_meta.parquet\"\n",
    "    meta_good_name: str = \"algos_meta_good.parquet\"\n",
    "    features_name: str = \"algos_features.parquet\"\n",
    "    features_good_name: str = \"algos_features_good.parquet\"\n",
    "    alive_intervals_name: str = \"alive_intervals.parquet\"\n",
    "    \n",
    "    benchmark_trades_name: str = \"benchmark_trades_clean.parquet\"\n",
    "    benchmark_monthly_name: str = \"benchmark_monthly_clean.parquet\"\n",
    "    benchmark_yearly_name: str = \"benchmark_yearly_clean.parquet\"\n",
    "    benchmark_monthly_stats_name: str = \"benchmark_monthly_stats.parquet\"\n",
    "\n",
    "def ensure_dirs(cfg: PreprocessConfig) -> None:\n",
    "    cfg.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (cfg.cache_dir / \"checks\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def algos_dir(cfg: PreprocessConfig) -> Path:\n",
    "    d = cfg.root_dir / cfg.algos_subdir\n",
    "    if not d.exists():\n",
    "        raise PreprocessError(f\"No existe la carpeta de algoritmos: {d}\")\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc044b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 2 — Helpers (robustos)\n",
    "# =========================\n",
    "\n",
    "def pick_first_existing(cols: Iterable[str], candidates: Sequence[str]) -> str:\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    raise PreprocessError(\n",
    "        f\"No encontré ninguna columna entre {candidates}. Columnas disponibles: {sorted(cols)[:30]}...\"\n",
    "    )\n",
    "\n",
    "def build_dt_expr(colname: str, cfg: PreprocessConfig) -> pl.Expr:\n",
    "    \"\"\"\n",
    "    Intenta parsear datetime con varios formatos.\n",
    "    Si falla, devuelve Null -> luego filtramos.\n",
    "    \"\"\"\n",
    "    exprs = [\n",
    "        pl.col(colname).str.strptime(pl.Datetime, format=fmt, strict=False)\n",
    "        for fmt in cfg.dt_format_candidates\n",
    "    ]\n",
    "    # coalesce => primer parseo válido\n",
    "    return pl.coalesce(exprs).alias(\"dt\")\n",
    "\n",
    "def safe_write_parquet(lf: pl.LazyFrame, path: Path, overwrite: bool = False) -> None:\n",
    "    if path.exists() and not overwrite:\n",
    "        return\n",
    "    lf.sink_parquet(str(path), compression=\"zstd\", statistics=True)\n",
    "\n",
    "def require_columns(df_or_lf, required: Sequence[str], name: str) -> None:\n",
    "    cols = df_or_lf.columns if hasattr(df_or_lf, \"columns\") else []\n",
    "    missing = [c for c in required if c not in cols]\n",
    "    if missing:\n",
    "        raise PreprocessError(f\"[{name}] faltan columnas: {missing}. Hay: {cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d619fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3 — Step A: Build panel (algo_id, date, close, ret_1d, logret_1d)\n",
    "# =========================\n",
    "\n",
    "def build_algos_panel(cfg: PreprocessConfig, overwrite: bool = False) -> Path:\n",
    "    ensure_dirs(cfg)\n",
    "    out_path = cfg.cache_dir / cfg.panel_name\n",
    "    if out_path.exists() and not overwrite:\n",
    "        return out_path\n",
    "\n",
    "    pattern = str(algos_dir(cfg) / \"*.csv\")\n",
    "\n",
    "    # Leemos una muestra mínima para detectar columnas reales (robusto)\n",
    "    sample_files = sorted(algos_dir(cfg).glob(\"*.csv\"))\n",
    "    if not sample_files:\n",
    "        raise PreprocessError(f\"No hay CSV en {algos_dir(cfg)}\")\n",
    "    sample = pl.read_csv(sample_files[0], n_rows=5)\n",
    "    dt_col = pick_first_existing(sample.columns, cfg.dt_candidates)\n",
    "    close_col = pick_first_existing(sample.columns, cfg.close_candidates)\n",
    "\n",
    "    # Lazy scan masivo + ruta para extraer algo_id\n",
    "    lf = pl.scan_csv(\n",
    "        pattern,\n",
    "        glob=True,\n",
    "        ignore_errors=True,              # robusto ante algún csv corrupto\n",
    "        infer_schema_length=0,           # no gastes tiempo infiriendo demasiado\n",
    "        include_file_paths=\"path\",\n",
    "    )\n",
    "\n",
    "    # Parseo dt robusto + casting close\n",
    "    # Regex mejorada para Windows y Unix: captura el nombre del archivo sin extensión\n",
    "    lf = (\n",
    "        lf\n",
    "        .with_columns([\n",
    "            # Extraer solo el nombre del archivo (funciona en Windows y Unix)\n",
    "            pl.col(\"path\").str.replace_all(r\".*[/\\\\]\", \"\").str.replace(r\"\\.csv$\", \"\").alias(\"algo_id\"),\n",
    "            build_dt_expr(dt_col, cfg),\n",
    "            pl.col(close_col).cast(pl.Float64, strict=False).alias(\"close_raw\"),\n",
    "        ])\n",
    "        .select([\"algo_id\", \"dt\", \"close_raw\"])\n",
    "        .filter(pl.col(\"algo_id\").is_not_null())\n",
    "        .filter(pl.col(\"algo_id\") != \"\")\n",
    "        .filter(pl.col(\"dt\").is_not_null())\n",
    "        .filter(pl.col(\"close_raw\").is_not_null())\n",
    "        .with_columns([\n",
    "            pl.col(\"dt\").dt.date().alias(\"date\"),\n",
    "            pl.col(\"close_raw\").alias(\"close\"),\n",
    "        ])\n",
    "        .select([\"algo_id\", \"date\", \"dt\", \"close\"])\n",
    "    )\n",
    "\n",
    "    # Normalización diaria (si hubiera intradía): último close del día\n",
    "    daily = (\n",
    "        lf\n",
    "        .sort([\"algo_id\", \"date\", \"dt\"])\n",
    "        .group_by([\"algo_id\", \"date\"], maintain_order=True)\n",
    "        .agg(pl.last(\"close\").alias(\"close\"))\n",
    "        .sort([\"algo_id\", \"date\"])\n",
    "        .with_columns([\n",
    "            pl.col(\"close\").pct_change().over(\"algo_id\").alias(\"ret_1d\"),\n",
    "            (pl.col(\"close\") / pl.col(\"close\").shift(1)).log().over(\"algo_id\").alias(\"logret_1d\"),\n",
    "        ])\n",
    "        # clipping robusto (protege features/RL de spikes absurdos)\n",
    "        .with_columns([\n",
    "            pl.col(\"ret_1d\").clip(-cfg.max_abs_ret_clip, cfg.max_abs_ret_clip).alias(\"ret_1d\"),\n",
    "            pl.col(\"logret_1d\").clip(-cfg.max_abs_ret_clip, cfg.max_abs_ret_clip).alias(\"logret_1d\"),\n",
    "        ])\n",
    "        .select([\"algo_id\", \"date\", \"close\", \"ret_1d\", \"logret_1d\"])\n",
    "    )\n",
    "\n",
    "    safe_write_parquet(daily, out_path, overwrite=True)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a287a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 4 — Step B: Meta por algoritmo + alive_intervals\n",
    "# =========================\n",
    "\n",
    "def build_algos_meta(cfg: PreprocessConfig, panel_path: Path, overwrite: bool = False) -> tuple[Path, Path]:\n",
    "    ensure_dirs(cfg)\n",
    "    out_meta = cfg.cache_dir / cfg.meta_name\n",
    "    out_alive = cfg.cache_dir / cfg.alive_intervals_name\n",
    "\n",
    "    if out_meta.exists() and out_alive.exists() and not overwrite:\n",
    "        return out_meta, out_alive\n",
    "\n",
    "    lf = pl.scan_parquet(str(panel_path))\n",
    "\n",
    "    ann = float(cfg.annualization_factor)\n",
    "\n",
    "    meta = (\n",
    "        lf\n",
    "        .group_by(\"algo_id\")\n",
    "        .agg([\n",
    "            pl.min(\"date\").alias(\"start_date\"),\n",
    "            pl.max(\"date\").alias(\"end_date\"),\n",
    "            pl.len().alias(\"n_obs\"),\n",
    "            pl.col(\"close\").n_unique().alias(\"n_unique_close\"),\n",
    "            pl.col(\"close\").std().alias(\"close_std\"),\n",
    "            pl.col(\"ret_1d\").mean().alias(\"ret_mean\"),\n",
    "            pl.col(\"ret_1d\").std().alias(\"ret_std\"),\n",
    "            pl.col(\"ret_1d\").median().alias(\"ret_median\"),\n",
    "            pl.col(\"ret_1d\").quantile(0.01).alias(\"ret_q01\"),\n",
    "            pl.col(\"ret_1d\").quantile(0.99).alias(\"ret_q99\"),\n",
    "            # Max drawdown (sobre close)\n",
    "            (pl.col(\"close\") / pl.col(\"close\").cum_max() - 1.0).min().alias(\"max_drawdown\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # días totales entre start y end (incl.)\n",
    "            (pl.col(\"end_date\") - pl.col(\"start_date\")).dt.total_days().cast(pl.Int64).alias(\"span_days\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"span_days\") + 1).alias(\"n_days\"),\n",
    "            (pl.col(\"n_obs\") / (pl.col(\"span_days\") + 1)).alias(\"coverage_ratio\"),\n",
    "            (pl.col(\"close_std\") <= cfg.constant_close_std_eps).alias(\"is_constant_std\"),\n",
    "            (pl.col(\"n_unique_close\") <= 2).alias(\"is_constant_unique\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"is_constant_std\") | pl.col(\"is_constant_unique\")).alias(\"is_constant\"),\n",
    "            # Sharpe anualizado (simple)\n",
    "            pl.when(pl.col(\"ret_std\") > 0)\n",
    "              .then((pl.col(\"ret_mean\") / pl.col(\"ret_std\")) * np.sqrt(ann))\n",
    "              .otherwise(None)\n",
    "              .alias(\"sharpe_ann\"),\n",
    "            (pl.col(\"ret_std\") * np.sqrt(ann)).alias(\"vol_ann\"),\n",
    "        ])\n",
    "        .select([\n",
    "            \"algo_id\", \"start_date\", \"end_date\",\n",
    "            \"n_obs\", \"n_days\", \"coverage_ratio\",\n",
    "            \"n_unique_close\", \"close_std\", \"is_constant\",\n",
    "            \"ret_mean\", \"ret_std\", \"ret_median\", \"ret_q01\", \"ret_q99\",\n",
    "            \"vol_ann\", \"sharpe_ann\", \"max_drawdown\",\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    safe_write_parquet(meta, out_meta, overwrite=True)\n",
    "\n",
    "    # Alive intervals (lo más eficiente para action-masking)\n",
    "    alive = (\n",
    "        pl.scan_parquet(str(out_meta))\n",
    "        .select([\"algo_id\", \"start_date\", \"end_date\"])\n",
    "    )\n",
    "    safe_write_parquet(alive, out_alive, overwrite=True)\n",
    "\n",
    "    return out_meta, out_alive\n",
    "\n",
    "\n",
    "def build_good_universe(cfg: PreprocessConfig, meta_path: Path, overwrite: bool = False) -> Path:\n",
    "    ensure_dirs(cfg)\n",
    "    out_good = cfg.cache_dir / cfg.meta_good_name\n",
    "    if out_good.exists() and not overwrite:\n",
    "        return out_good\n",
    "\n",
    "    good = (\n",
    "        pl.scan_parquet(str(meta_path))\n",
    "        .filter(pl.col(\"n_obs\") >= cfg.min_obs)\n",
    "        .filter(pl.col(\"coverage_ratio\") >= cfg.min_coverage)\n",
    "        .filter(~pl.col(\"is_constant\"))\n",
    "    )\n",
    "    safe_write_parquet(good, out_good, overwrite=True)\n",
    "    return out_good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c633600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 5 — Step C: Features RL (rolling) por día y algoritmo\n",
    "# =========================\n",
    "\n",
    "def build_features(cfg: PreprocessConfig, panel_path: Path, meta_good_path: Path | None = None, overwrite: bool = False) -> tuple[Path, Path]:\n",
    "    \"\"\"\n",
    "    Genera:\n",
    "      - algos_features.parquet (todos)\n",
    "      - algos_features_good.parquet (solo good universe)\n",
    "    \"\"\"\n",
    "    ensure_dirs(cfg)\n",
    "    out_all = cfg.cache_dir / cfg.features_name\n",
    "    out_good = cfg.cache_dir / cfg.features_good_name\n",
    "\n",
    "    if out_all.exists() and out_good.exists() and not overwrite:\n",
    "        return out_all, out_good\n",
    "\n",
    "    ann_sqrt = float(np.sqrt(cfg.annualization_factor))\n",
    "\n",
    "    base = (\n",
    "        pl.scan_parquet(str(panel_path))\n",
    "        .sort([\"algo_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    # Rolling features (solo con ret_1d y close)\n",
    "    feats = base\n",
    "    for w in cfg.feature_windows:\n",
    "        feats = feats.with_columns([\n",
    "            pl.col(\"ret_1d\").rolling_mean(window_size=w, min_samples=w).over(\"algo_id\").alias(f\"ret_mean_{w}\"),\n",
    "            pl.col(\"ret_1d\").rolling_std(window_size=w, min_samples=w).over(\"algo_id\").alias(f\"ret_std_{w}\"),\n",
    "            pl.col(\"ret_1d\").rolling_sum(window_size=w, min_samples=w).over(\"algo_id\").alias(f\"ret_sum_{w}\"),\n",
    "        ]).with_columns([\n",
    "            (pl.col(f\"ret_std_{w}\") * ann_sqrt).alias(f\"vol_ann_{w}\"),\n",
    "            pl.when(pl.col(f\"ret_std_{w}\") > 0)\n",
    "              .then((pl.col(f\"ret_mean_{w}\") / pl.col(f\"ret_std_{w}\")) * ann_sqrt)\n",
    "              .otherwise(None)\n",
    "              .alias(f\"sharpe_ann_{w}\"),\n",
    "        ])\n",
    "\n",
    "    # Extra features útiles y baratas:\n",
    "    feats = feats.with_columns([\n",
    "        # \"price momentum\" (log close - log close lag w) para el mayor window\n",
    "        (pl.col(\"close\").log() - pl.col(\"close\").shift(cfg.feature_windows[-1]).log())\n",
    "        .over(\"algo_id\")\n",
    "        .alias(f\"log_mom_{cfg.feature_windows[-1]}\"),\n",
    "    ])\n",
    "\n",
    "    # Selección final - logret_1d viene del panel base, aseguramos que esté disponible\n",
    "    keep_cols = [\"algo_id\", \"date\", \"close\", \"ret_1d\", \"logret_1d\"] + \\\n",
    "               [f\"ret_mean_{w}\" for w in cfg.feature_windows] + \\\n",
    "               [f\"ret_std_{w}\" for w in cfg.feature_windows] + \\\n",
    "               [f\"ret_sum_{w}\" for w in cfg.feature_windows] + \\\n",
    "               [f\"vol_ann_{w}\" for w in cfg.feature_windows] + \\\n",
    "               [f\"sharpe_ann_{w}\" for w in cfg.feature_windows] + \\\n",
    "               [f\"log_mom_{cfg.feature_windows[-1]}\"]\n",
    "\n",
    "    # Verificamos qué columnas realmente existen antes de seleccionar\n",
    "    available_cols = feats.collect_schema().names()\n",
    "    keep_cols = [c for c in keep_cols if c in available_cols]\n",
    "    \n",
    "    feats_all = feats.select(keep_cols)\n",
    "    safe_write_parquet(feats_all, out_all, overwrite=True)\n",
    "\n",
    "    # Filtrado a universo \"good\"\n",
    "    if meta_good_path is None:\n",
    "        raise PreprocessError(\"meta_good_path es None, pero se necesita para features_good.\")\n",
    "    good_ids = pl.scan_parquet(str(meta_good_path)).select([\"algo_id\"])\n",
    "    feats_good = feats_all.join(good_ids, on=\"algo_id\", how=\"inner\")\n",
    "    safe_write_parquet(feats_good, out_good, overwrite=True)\n",
    "\n",
    "    return out_all, out_good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e854db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 6 — Step D: Benchmark preprocessing (trades + returns)\n",
    "# =========================\n",
    "\n",
    "def preprocess_benchmark(cfg: PreprocessConfig, overwrite: bool = False) -> dict[str, Path]:\n",
    "    ensure_dirs(cfg)\n",
    "\n",
    "    trades_csv = cfg.root_dir / \"trades_benchmark.csv\"\n",
    "    monthly_csv = cfg.root_dir / \"benchmark_monthly_returns.csv\"\n",
    "    yearly_csv = cfg.root_dir / \"benchmark_yearly_returns.csv\"\n",
    "\n",
    "    if not trades_csv.exists():\n",
    "        raise PreprocessError(f\"No existe {trades_csv}\")\n",
    "    if not monthly_csv.exists():\n",
    "        raise PreprocessError(f\"No existe {monthly_csv}\")\n",
    "    if not yearly_csv.exists():\n",
    "        raise PreprocessError(f\"No existe {yearly_csv}\")\n",
    "\n",
    "    out_trades = cfg.cache_dir / cfg.benchmark_trades_name\n",
    "    out_monthly = cfg.cache_dir / cfg.benchmark_monthly_name\n",
    "    out_yearly = cfg.cache_dir / cfg.benchmark_yearly_name\n",
    "    out_monthly_stats = cfg.cache_dir / cfg.benchmark_monthly_stats_name\n",
    "\n",
    "    # -------- trades_benchmark (limpio) --------\n",
    "    if not out_trades.exists() or overwrite:\n",
    "        t = pl.scan_csv(str(trades_csv), infer_schema_length=0)\n",
    "\n",
    "        # Fechas robustas: extraemos YYYY-MM-DD directamente (evita problemas de tz)\n",
    "        t = (\n",
    "            t\n",
    "            .with_columns([\n",
    "                pl.col(\"dateOpen\").cast(pl.Utf8).str.slice(0, 10).alias(\"open_date_str\"),\n",
    "                pl.col(\"dateClose\").cast(pl.Utf8).str.slice(0, 10).alias(\"close_date_str\"),\n",
    "            ])\n",
    "            .with_columns([\n",
    "                pl.col(\"open_date_str\").str.strptime(pl.Date, \"%Y-%m-%d\", strict=False).alias(\"open_date\"),\n",
    "                pl.col(\"close_date_str\").str.strptime(pl.Date, \"%Y-%m-%d\", strict=False).alias(\"close_date\"),\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col(\"close_date\") - pl.col(\"open_date\")).dt.total_days().cast(pl.Int64).alias(\"holding_days\"),\n",
    "            ])\n",
    "            .with_columns([\n",
    "                pl.col(\"volume\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"AUM\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"equity_EOD\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"equity_normalized\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"productname\").cast(pl.Utf8),\n",
    "            ])\n",
    "            .select([\n",
    "                \"productname\",\n",
    "                \"volume\",\n",
    "                \"open_date\",\n",
    "                \"close_date\",\n",
    "                \"holding_days\",\n",
    "                \"total_invested_amount_EOD\",\n",
    "                \"equity_EOD\",\n",
    "                \"AUM\",\n",
    "                \"equity_normalized\",\n",
    "                \"dateOpen\",\n",
    "                \"dateClose\",\n",
    "            ])\n",
    "            .filter(pl.col(\"open_date\").is_not_null())\n",
    "            .filter(pl.col(\"close_date\").is_not_null())\n",
    "        )\n",
    "\n",
    "        safe_write_parquet(t, out_trades, overwrite=True)\n",
    "\n",
    "    # -------- monthly returns (clean + consistency check) --------\n",
    "    if not out_monthly.exists() or overwrite:\n",
    "        m = (\n",
    "            pl.scan_csv(str(monthly_csv), infer_schema_length=0)\n",
    "            .with_columns([\n",
    "                pl.col(\"month\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y-%m\", strict=False).alias(\"month_date\"),\n",
    "                pl.col(\"start_equity\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"end_equity\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"monthly_return\").cast(pl.Float64, strict=False),\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col(\"end_equity\") / pl.col(\"start_equity\") - 1.0).alias(\"monthly_return_calc\"),\n",
    "                (pl.col(\"monthly_return\") - (pl.col(\"end_equity\") / pl.col(\"start_equity\") - 1.0)).abs().alias(\"monthly_return_abs_err\"),\n",
    "            ])\n",
    "            .select([\"month_date\", \"start_equity\", \"end_equity\", \"monthly_return\", \"monthly_return_calc\", \"monthly_return_abs_err\"])\n",
    "        )\n",
    "        safe_write_parquet(m, out_monthly, overwrite=True)\n",
    "\n",
    "    # -------- yearly returns (clean + consistency check) --------\n",
    "    if not out_yearly.exists() or overwrite:\n",
    "        y = pl.scan_csv(str(yearly_csv), infer_schema_length=0)\n",
    "\n",
    "        # Intento robusto: si viene columna \"year\" o similar, lo convertimos a date 1-ene\n",
    "        cols = y.columns\n",
    "        year_col = None\n",
    "        for c in (\"year\", \"Year\", \"anio\", \"año\"):\n",
    "            if c in cols:\n",
    "                year_col = c\n",
    "                break\n",
    "\n",
    "        if year_col is None:\n",
    "            # Si no hay, lo dejamos tal cual y casteamos numéricos si existen\n",
    "            y2 = y\n",
    "        else:\n",
    "            y2 = (\n",
    "                y.with_columns([\n",
    "                    pl.col(year_col).cast(pl.Int32, strict=False).alias(\"year_int\"),\n",
    "                    pl.date(pl.col(year_col).cast(pl.Int32, strict=False), 1, 1).alias(\"year_date\"),\n",
    "                ])\n",
    "            )\n",
    "\n",
    "        # Casteos típicos si están\n",
    "        for c in (\"start_equity\", \"end_equity\", \"yearly_return\"):\n",
    "            if c in y2.columns:\n",
    "                y2 = y2.with_columns(pl.col(c).cast(pl.Float64, strict=False))\n",
    "\n",
    "        if \"start_equity\" in y2.columns and \"end_equity\" in y2.columns and \"yearly_return\" in y2.columns:\n",
    "            y2 = y2.with_columns([\n",
    "                (pl.col(\"end_equity\") / pl.col(\"start_equity\") - 1.0).alias(\"yearly_return_calc\"),\n",
    "                (pl.col(\"yearly_return\") - (pl.col(\"end_equity\") / pl.col(\"start_equity\") - 1.0)).abs().alias(\"yearly_return_abs_err\"),\n",
    "            ])\n",
    "\n",
    "        safe_write_parquet(y2, out_yearly, overwrite=True)\n",
    "\n",
    "    # -------- stats mensuales del benchmark (desde trades) --------\n",
    "    if not out_monthly_stats.exists() or overwrite:\n",
    "        trades = pl.scan_parquet(str(out_trades))\n",
    "\n",
    "        stats = (\n",
    "            trades\n",
    "            .with_columns([\n",
    "                pl.col(\"open_date\").dt.truncate(\"1mo\").alias(\"month\"),\n",
    "                pl.col(\"close_date\").dt.truncate(\"1mo\").alias(\"month_close\"),\n",
    "            ])\n",
    "            .group_by(\"month\")\n",
    "            .agg([\n",
    "                pl.len().alias(\"n_trades_opened\"),\n",
    "                pl.col(\"productname\").n_unique().alias(\"unique_algos_opened\"),\n",
    "                pl.col(\"volume\").sum().alias(\"sum_volume_opened\"),\n",
    "                pl.col(\"holding_days\").mean().alias(\"avg_holding_days_opened\"),\n",
    "                pl.col(\"AUM\").mean().alias(\"avg_AUM\"),\n",
    "                pl.col(\"equity_normalized\").mean().alias(\"avg_equity_norm\"),\n",
    "            ])\n",
    "            .sort(\"month\")\n",
    "        )\n",
    "\n",
    "        safe_write_parquet(stats, out_monthly_stats, overwrite=True)\n",
    "\n",
    "    return {\n",
    "        \"benchmark_trades_clean\": out_trades,\n",
    "        \"benchmark_monthly_clean\": out_monthly,\n",
    "        \"benchmark_yearly_clean\": out_yearly,\n",
    "        \"benchmark_monthly_stats\": out_monthly_stats,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd62013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 7 — Sanity checks (rápidos y útiles)\n",
    "# =========================\n",
    "\n",
    "def sanity_checks(cfg: PreprocessConfig, panel_path: Path, meta_path: Path, meta_good_path: Path) -> None:\n",
    "    panel = pl.scan_parquet(str(panel_path))\n",
    "    meta = pl.scan_parquet(str(meta_path))\n",
    "    good = pl.scan_parquet(str(meta_good_path))\n",
    "\n",
    "    # Chequeos básicos\n",
    "    panel_cols = panel.collect_schema().names()\n",
    "    needed_panel = [\"algo_id\", \"date\", \"close\", \"ret_1d\"]  # logret_1d es opcional\n",
    "    for c in needed_panel:\n",
    "        if c not in panel_cols:\n",
    "            raise PreprocessError(f\"Panel no tiene columna requerida: {c}. Tiene: {panel_cols}\")\n",
    "\n",
    "    # Resumen rápido (collect pequeño)\n",
    "    summary = meta.select([\n",
    "        pl.len().alias(\"n_algos\"),\n",
    "        pl.col(\"n_obs\").mean().alias(\"avg_n_obs\"),\n",
    "        pl.col(\"coverage_ratio\").mean().alias(\"avg_coverage\"),\n",
    "        pl.col(\"is_constant\").mean().alias(\"pct_constant\"),\n",
    "        pl.col(\"vol_ann\").median().alias(\"median_vol_ann\"),\n",
    "        pl.col(\"sharpe_ann\").median().alias(\"median_sharpe_ann\"),\n",
    "    ]).collect()\n",
    "\n",
    "    good_summary = good.select([\n",
    "        pl.len().alias(\"n_good_algos\"),\n",
    "        pl.col(\"n_obs\").mean().alias(\"avg_n_obs_good\"),\n",
    "        pl.col(\"coverage_ratio\").mean().alias(\"avg_coverage_good\"),\n",
    "    ]).collect()\n",
    "\n",
    "    print(\"=== META SUMMARY ===\")\n",
    "    print(summary)\n",
    "    print(\"\\n=== GOOD UNIVERSE SUMMARY ===\")\n",
    "    print(good_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "613afc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 8 — Orquestación (run)\n",
    "# =========================\n",
    "\n",
    "def run_preprocessing(cfg: PreprocessConfig, overwrite: bool = False) -> dict[str, Path]:\n",
    "    ensure_dirs(cfg)\n",
    "\n",
    "    # A) Panel\n",
    "    panel_path = build_algos_panel(cfg, overwrite=overwrite)\n",
    "\n",
    "    # B) Meta + alive intervals\n",
    "    meta_path, alive_path = build_algos_meta(cfg, panel_path, overwrite=overwrite)\n",
    "    meta_good_path = build_good_universe(cfg, meta_path, overwrite=overwrite)\n",
    "\n",
    "    # C) Features RL\n",
    "    feats_all_path, feats_good_path = build_features(\n",
    "        cfg,\n",
    "        panel_path=panel_path,\n",
    "        meta_good_path=meta_good_path,\n",
    "        overwrite=overwrite\n",
    "    )\n",
    "\n",
    "    # D) Benchmark\n",
    "    bench_paths = preprocess_benchmark(cfg, overwrite=overwrite)\n",
    "\n",
    "    # Checks\n",
    "    sanity_checks(cfg, panel_path, meta_path, meta_good_path)\n",
    "\n",
    "    return {\n",
    "        \"algos_panel\": panel_path,\n",
    "        \"algos_meta\": meta_path,\n",
    "        \"algos_meta_good\": meta_good_path,\n",
    "        \"alive_intervals\": alive_path,\n",
    "        \"algos_features\": feats_all_path,\n",
    "        \"algos_features_good\": feats_good_path,\n",
    "        **bench_paths,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff6625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== META SUMMARY ===\n",
      "shape: (1, 6)\n",
      "┌─────────┬────────────┬──────────────┬──────────────┬────────────────┬───────────────────┐\n",
      "│ n_algos ┆ avg_n_obs  ┆ avg_coverage ┆ pct_constant ┆ median_vol_ann ┆ median_sharpe_ann │\n",
      "│ ---     ┆ ---        ┆ ---          ┆ ---          ┆ ---            ┆ ---               │\n",
      "│ u32     ┆ f64        ┆ f64          ┆ f64          ┆ f64            ┆ f64               │\n",
      "╞═════════╪════════════╪══════════════╪══════════════╪════════════════╪═══════════════════╡\n",
      "│ 13663   ┆ 649.866062 ┆ 0.839531     ┆ 0.247896     ┆ 0.055688       ┆ -0.398133         │\n",
      "└─────────┴────────────┴──────────────┴──────────────┴────────────────┴───────────────────┘\n",
      "\n",
      "=== GOOD UNIVERSE SUMMARY ===\n",
      "shape: (1, 3)\n",
      "┌──────────────┬────────────────┬───────────────────┐\n",
      "│ n_good_algos ┆ avg_n_obs_good ┆ avg_coverage_good │\n",
      "│ ---          ┆ ---            ┆ ---               │\n",
      "│ u32          ┆ f64            ┆ f64               │\n",
      "╞══════════════╪════════════════╪═══════════════════╡\n",
      "│ 9491         ┆ 735.905279     ┆ 0.840602          │\n",
      "└──────────────┴────────────────┴───────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_12280\\2469311116.py:91: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  cols = y.columns\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_12280\\2469311116.py:111: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if c in y2.columns:\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_12280\\2469311116.py:114: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if \"start_equity\" in y2.columns and \"end_equity\" in y2.columns and \"yearly_return\" in y2.columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'algos_panel': WindowsPath('cache/algos_panel.parquet'),\n",
       " 'algos_meta': WindowsPath('cache/algos_meta.parquet'),\n",
       " 'algos_meta_good': WindowsPath('cache/algos_meta_good.parquet'),\n",
       " 'alive_intervals': WindowsPath('cache/alive_intervals.parquet'),\n",
       " 'algos_features': WindowsPath('cache/algos_features.parquet'),\n",
       " 'algos_features_good': WindowsPath('cache/algos_features_good.parquet'),\n",
       " 'benchmark_trades_clean': WindowsPath('cache/benchmark_trades_clean.parquet'),\n",
       " 'benchmark_monthly_clean': WindowsPath('cache/benchmark_monthly_clean.parquet'),\n",
       " 'benchmark_yearly_clean': WindowsPath('cache/benchmark_yearly_clean.parquet'),\n",
       " 'benchmark_monthly_stats': WindowsPath('cache/benchmark_monthly_stats.parquet')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 9 — Ejecuta (ajusta root_dir)\n",
    "# =========================\n",
    "\n",
    "cfg = PreprocessConfig(\n",
    "    root_dir=Path(\"datos_competicion\"),\n",
    "    cache_dir=Path(\"data/cache\"),\n",
    "    min_obs=60,\n",
    "    min_coverage=0.70,\n",
    "    feature_windows=(20, 60, 120),\n",
    ")\n",
    "\n",
    "paths = run_preprocessing(cfg, overwrite=True)  # overwrite=True para regenerar\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c450de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # Cell 9 — Ejecuta (ajusta root_dir)\n",
    "# # =========================\n",
    "\n",
    "# cfg = PreprocessConfig(\n",
    "#     root_dir=Path(\"datos_competicion\"),\n",
    "#     cache_dir=Path(\"cache\"),\n",
    "#     min_obs=60,\n",
    "#     min_coverage=0.70,\n",
    "#     feature_windows=(20, 60, 120),\n",
    "# )\n",
    "\n",
    "# paths = run_preprocessing(cfg, overwrite=False)\n",
    "# paths\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
