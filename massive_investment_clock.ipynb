{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e9ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "pl.enable_string_cache()\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_cols(20)\n",
    "\n",
    "class MacroError(RuntimeError):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cbd32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -olars-runtime-32 (c:\\users\\maria\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -olars-runtime-32 (c:\\users\\maria\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -olars-runtime-32 (c:\\users\\maria\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -olars-runtime-32 (c:\\users\\maria\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -olars-runtime-32 (c:\\users\\maria\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -olars-runtime-32 (c:\\users\\maria\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7328a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>inflation_yoy</th><th>growth_yoy</th></tr><tr><td>date</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2020-01-01</td><td>2.599768</td><td>-2.290379</td></tr><tr><td>2020-02-01</td><td>2.341317</td><td>-1.423222</td></tr><tr><td>2020-03-01</td><td>1.49404</td><td>-5.315055</td></tr><tr><td>2020-04-01</td><td>0.313047</td><td>-17.318282</td></tr><tr><td>2020-05-01</td><td>0.198201</td><td>-16.047738</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌────────────┬───────────────┬────────────┐\n",
       "│ date       ┆ inflation_yoy ┆ growth_yoy │\n",
       "│ ---        ┆ ---           ┆ ---        │\n",
       "│ date       ┆ f64           ┆ f64        │\n",
       "╞════════════╪═══════════════╪════════════╡\n",
       "│ 2020-01-01 ┆ 2.599768      ┆ -2.290379  │\n",
       "│ 2020-02-01 ┆ 2.341317      ┆ -1.423222  │\n",
       "│ 2020-03-01 ┆ 1.49404       ┆ -5.315055  │\n",
       "│ 2020-04-01 ┆ 0.313047      ┆ -17.318282 │\n",
       "│ 2020-05-01 ┆ 0.198201      ┆ -16.047738 │\n",
       "└────────────┴───────────────┴────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import polars as pl\n",
    "\n",
    "def fred_series_to_df(series_id: str, api_key: str, start: str = \"2019-01-01\") -> pl.DataFrame:\n",
    "    url = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": api_key,\n",
    "        \"file_type\": \"json\",\n",
    "        \"observation_start\": start,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    obs = r.json()[\"observations\"]\n",
    "    df = pl.DataFrame({\n",
    "        \"date\": [o[\"date\"] for o in obs],\n",
    "        series_id: [None if o[\"value\"] == \".\" else float(o[\"value\"]) for o in obs],\n",
    "    }).with_columns(pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\"))\n",
    "    return df.drop_nulls()\n",
    "\n",
    "def yoy_from_index(df: pl.DataFrame, col: str) -> pl.DataFrame:\n",
    "    # mensual -> YoY = (x / x.shift(12) - 1)*100\n",
    "    return (\n",
    "        df.sort(\"date\")\n",
    "          .with_columns(((pl.col(col) / pl.col(col).shift(12) - 1.0) * 100.0).alias(f\"{col}_yoy\"))\n",
    "          .drop_nulls()\n",
    "    )\n",
    "\n",
    "FRED_API_KEY = \"6e425839b9859cd25c0b054cd074fb4d\"\n",
    "\n",
    "cpi = fred_series_to_df(\"CPIAUCSL\", FRED_API_KEY)\n",
    "indpro = fred_series_to_df(\"INDPRO\", FRED_API_KEY)\n",
    "\n",
    "infl = yoy_from_index(cpi, \"CPIAUCSL\").select([\"date\", pl.col(\"CPIAUCSL_yoy\").alias(\"inflation_yoy\")])\n",
    "grow = yoy_from_index(indpro, \"INDPRO\").select([\"date\", pl.col(\"INDPRO_yoy\").alias(\"growth_yoy\")])\n",
    "\n",
    "macro = infl.join(grow, on=\"date\", how=\"inner\").sort(\"date\")\n",
    "macro.write_csv(\"macro_data.csv\")\n",
    "macro.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class MacroConfig:\n",
    "    cache_dir: Path = Path(\"data/cache\")\n",
    "\n",
    "    # Inputs creados previamente\n",
    "    algos_features_good: Path = Path(\"data/cache/algos_features_good.parquet\")\n",
    "    benchmark_trades_clean: Path = Path(\"data/cache/benchmark_trades_clean.parquet\")\n",
    "    # INPUT EXTERNO (tuyo)\n",
    "    macro_csv: Path = Path(\"data/datos_competicion/macro_data.csv\")  # <-- pon aquí tu fichero macro\n",
    "\n",
    "    # Columnas mínimas\n",
    "    date_col: str = \"date\"\n",
    "    inflation_col: str = \"inflation_yoy\"\n",
    "    growth_col: str = \"growth_yoy\"\n",
    "\n",
    "    # Investment Clock: cómo definimos \"alto/bajo\" sin mirar el futuro\n",
    "    # rolling_median: usa mediana móvil histórica (shifted 1) para thresholding\n",
    "    regime_method: str = \"rolling_median\"   # \"rolling_median\" | \"expanding_median\" | \"zscore\"\n",
    "    window_months: int = 36                 # para rolling_median (si data mensual)\n",
    "    min_months: int = 12\n",
    "\n",
    "    # Evitar look-ahead por retraso de publicación (si quieres)\n",
    "    # Ejemplo: si macro es mensual y se publica con lag, pon lag_months=1\n",
    "    lag_months: int = 0\n",
    "\n",
    "    # Outputs\n",
    "    macro_states_parquet: Path = Path(\"data/cache/macro_states.parquet\")\n",
    "    algos_features_macro_parquet: Path = Path(\"data/cache/algos_features_macro.parquet\")\n",
    "\n",
    "    regime_profiles_long: Path = Path(\"data/cache/algo_regime_profiles_long.parquet\")\n",
    "    regime_profiles_wide: Path = Path(\"data/cache/algo_regime_profiles_wide.parquet\")\n",
    "    regime_rankings: Path = Path(\"data/cache/algo_regime_rankings.parquet\")\n",
    "\n",
    "    clusters_parquet: Path = Path(\"data/cache/algo_regime_clusters.parquet\")\n",
    "\n",
    "def _ensure_cache(cfg: MacroConfig) -> None:\n",
    "    cfg.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (cfg.cache_dir / \"checks\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9e754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_date_any(x: pl.Expr) -> pl.Expr:\n",
    "    # Soporta YYYY-MM-DD y YYYY-MM\n",
    "    return pl.coalesce([\n",
    "        x.str.strptime(pl.Date, \"%Y-%m-%d\", strict=False),\n",
    "        x.str.strptime(pl.Date, \"%Y-%m\", strict=False),\n",
    "    ])\n",
    "\n",
    "def load_macro(cfg: MacroConfig) -> pl.LazyFrame:\n",
    "    _ensure_cache(cfg)\n",
    "    if not cfg.macro_csv.exists():\n",
    "        raise MacroError(f\"No existe macro_csv: {cfg.macro_csv} (pon tu fichero macro externo ahí).\")\n",
    "\n",
    "    lf = pl.scan_csv(str(cfg.macro_csv), infer_schema_length=0)\n",
    "\n",
    "    cols = lf.columns\n",
    "    for c in (cfg.date_col, cfg.inflation_col, cfg.growth_col):\n",
    "        if c not in cols:\n",
    "            raise MacroError(f\"macro_csv debe tener columna '{c}'. Columnas: {cols}\")\n",
    "\n",
    "    lf = (\n",
    "        lf\n",
    "        .with_columns([\n",
    "            pl.col(cfg.date_col).cast(pl.Utf8).alias(\"date_str\"),\n",
    "            _parse_date_any(pl.col(cfg.date_col).cast(pl.Utf8)).alias(\"date\"),\n",
    "            pl.col(cfg.inflation_col).cast(pl.Float64, strict=False).alias(\"inflation_yoy\"),\n",
    "            pl.col(cfg.growth_col).cast(pl.Float64, strict=False).alias(\"growth_yoy\"),\n",
    "        ])\n",
    "        .filter(pl.col(\"date\").is_not_null())\n",
    "        .filter(pl.col(\"inflation_yoy\").is_not_null())\n",
    "        .filter(pl.col(\"growth_yoy\").is_not_null())\n",
    "        .sort(\"date\")\n",
    "    )\n",
    "\n",
    "    # Normaliza % vs decimal (si viene tipo 3.2 => 3.2%, si viene 0.032 => 3.2%)\n",
    "    # Heurística: si abs(media) < 0.5 asumimos decimal.\n",
    "    lf_stats = lf.select([\n",
    "        pl.col(\"inflation_yoy\").mean().alias(\"infl_mu\"),\n",
    "        pl.col(\"growth_yoy\").mean().alias(\"grow_mu\"),\n",
    "    ])\n",
    "    stats = lf_stats.collect()\n",
    "    infl_mu = float(stats[\"infl_mu\"][0]) if stats[\"infl_mu\"][0] is not None else 0.0\n",
    "    grow_mu = float(stats[\"grow_mu\"][0]) if stats[\"grow_mu\"][0] is not None else 0.0\n",
    "\n",
    "    infl_scale = 100.0 if abs(infl_mu) < 0.5 else 1.0\n",
    "    grow_scale = 100.0 if abs(grow_mu) < 0.5 else 1.0\n",
    "\n",
    "    lf = lf.with_columns([\n",
    "        (pl.col(\"inflation_yoy\") * infl_scale).alias(\"inflation_yoy\"),\n",
    "        (pl.col(\"growth_yoy\") * grow_scale).alias(\"growth_yoy\"),\n",
    "    ])\n",
    "\n",
    "    # Opcional: lag por publicación (evita look-ahead si el macro se publica con retraso)\n",
    "    if cfg.lag_months > 0:\n",
    "        lf = lf.with_columns([\n",
    "            pl.col(\"inflation_yoy\").shift(cfg.lag_months).alias(\"inflation_yoy\"),\n",
    "            pl.col(\"growth_yoy\").shift(cfg.lag_months).alias(\"growth_yoy\"),\n",
    "        ]).filter(pl.col(\"inflation_yoy\").is_not_null()).filter(pl.col(\"growth_yoy\").is_not_null())\n",
    "\n",
    "    return lf\n",
    "\n",
    "def compute_investment_clock_states(cfg: MacroConfig, macro_lf: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Devuelve: date, inflation_yoy, growth_yoy, regime_id, regime_name, one-hot, + macro continuas.\n",
    "    Sin fuga temporal: thresholds shifted (t-1).\n",
    "    \"\"\"\n",
    "    # Detectar frecuencia aproximada (mensual vs diaria) -> para window en filas, asumimos mensual\n",
    "    # Si tu macro es diaria, puedes bajar window_months o cambiar a \"expanding_median\".\n",
    "    W = int(cfg.window_months)\n",
    "    MINP = int(cfg.min_months)\n",
    "\n",
    "    if cfg.regime_method == \"rolling_median\":\n",
    "        infl_thr = pl.col(\"inflation_yoy\").rolling_median(W, min_periods=MINP).shift(1)\n",
    "        grow_thr = pl.col(\"growth_yoy\").rolling_median(W, min_periods=MINP).shift(1)\n",
    "    elif cfg.regime_method == \"expanding_median\":\n",
    "        # expanding median no está directo en polars; aproximamos con quantile expanding usando cumcount+window grande\n",
    "        # aquí usamos rolling con ventana enorme como proxy\n",
    "        BIG = 10_000\n",
    "        infl_thr = pl.col(\"inflation_yoy\").rolling_median(BIG, min_periods=MINP).shift(1)\n",
    "        grow_thr = pl.col(\"growth_yoy\").rolling_median(BIG, min_periods=MINP).shift(1)\n",
    "    elif cfg.regime_method == \"zscore\":\n",
    "        # zscore rolling: high si z > 0\n",
    "        infl_mu = pl.col(\"inflation_yoy\").rolling_mean(W, min_periods=MINP).shift(1)\n",
    "        infl_sd = pl.col(\"inflation_yoy\").rolling_std(W, min_periods=MINP).shift(1)\n",
    "        grow_mu = pl.col(\"growth_yoy\").rolling_mean(W, min_periods=MINP).shift(1)\n",
    "        grow_sd = pl.col(\"growth_yoy\").rolling_std(W, min_periods=MINP).shift(1)\n",
    "        infl_thr = infl_mu\n",
    "        grow_thr = grow_mu\n",
    "        # high = value > mean (z>0); sd se usa si quieres z explícito\n",
    "    else:\n",
    "        raise MacroError(f\"regime_method inválido: {cfg.regime_method}\")\n",
    "\n",
    "    base = macro_lf.with_columns([\n",
    "        infl_thr.alias(\"infl_thr\"),\n",
    "        grow_thr.alias(\"grow_thr\"),\n",
    "    ]).filter(pl.col(\"infl_thr\").is_not_null()).filter(pl.col(\"grow_thr\").is_not_null())\n",
    "\n",
    "    base = base.with_columns([\n",
    "        (pl.col(\"inflation_yoy\") > pl.col(\"infl_thr\")).alias(\"infl_high\"),\n",
    "        (pl.col(\"growth_yoy\") > pl.col(\"grow_thr\")).alias(\"grow_high\"),\n",
    "    ])\n",
    "\n",
    "    # Regime id mapping:\n",
    "    # 0 Reflation   (grow_low, infl_low)\n",
    "    # 1 Recovery    (grow_high, infl_low)\n",
    "    # 2 Overheat    (grow_high, infl_high)\n",
    "    # 3 Stagflation (grow_low, infl_high)\n",
    "    base = base.with_columns([\n",
    "        pl.when(pl.col(\"grow_high\") & pl.col(\"infl_high\")).then(pl.lit(2))\n",
    "         .when(pl.col(\"grow_high\") & ~pl.col(\"infl_high\")).then(pl.lit(1))\n",
    "         .when(~pl.col(\"grow_high\") & pl.col(\"infl_high\")).then(pl.lit(3))\n",
    "         .otherwise(pl.lit(0))\n",
    "         .alias(\"regime_id\"),\n",
    "    ]).with_columns([\n",
    "        pl.when(pl.col(\"regime_id\") == 0).then(pl.lit(\"Reflation\"))\n",
    "         .when(pl.col(\"regime_id\") == 1).then(pl.lit(\"Recovery\"))\n",
    "         .when(pl.col(\"regime_id\") == 2).then(pl.lit(\"Overheat\"))\n",
    "         .otherwise(pl.lit(\"Stagflation\"))\n",
    "         .alias(\"regime_name\"),\n",
    "    ])\n",
    "\n",
    "    # One-hot\n",
    "    base = base.with_columns([\n",
    "        (pl.col(\"regime_id\") == 0).cast(pl.Int8).alias(\"reg_reflation\"),\n",
    "        (pl.col(\"regime_id\") == 1).cast(pl.Int8).alias(\"reg_recovery\"),\n",
    "        (pl.col(\"regime_id\") == 2).cast(pl.Int8).alias(\"reg_overheat\"),\n",
    "        (pl.col(\"regime_id\") == 3).cast(pl.Int8).alias(\"reg_stagflation\"),\n",
    "    ])\n",
    "\n",
    "    # Añade automáticamente otras columnas macro numéricas (si existen)\n",
    "    # (y excluye las internas)\n",
    "    exclude = {\"date_str\", cfg.date_col, \"date\", \"inflation_yoy\", \"growth_yoy\", \"infl_thr\", \"grow_thr\",\n",
    "               \"infl_high\", \"grow_high\", \"regime_id\", \"regime_name\",\n",
    "               \"reg_reflation\", \"reg_recovery\", \"reg_overheat\", \"reg_stagflation\"}\n",
    "    num_cols = [c for c in macro_lf.columns if c not in exclude]\n",
    "\n",
    "    # casteamos numéricos extra a Float64 cuando sea posible\n",
    "    extra_exprs = []\n",
    "    for c in num_cols:\n",
    "        extra_exprs.append(pl.col(c).cast(pl.Float64, strict=False).alias(c))\n",
    "\n",
    "    out = base\n",
    "    if extra_exprs:\n",
    "        out = out.with_columns(extra_exprs)\n",
    "\n",
    "    keep = [\"date\", \"inflation_yoy\", \"growth_yoy\",\n",
    "            \"regime_id\", \"regime_name\",\n",
    "            \"reg_reflation\", \"reg_recovery\", \"reg_overheat\", \"reg_stagflation\"] + num_cols\n",
    "\n",
    "    return out.select([c for c in keep if c in out.columns])\n",
    "\n",
    "def write_macro_states(cfg: MacroConfig, states_lf: pl.LazyFrame, overwrite: bool = False) -> Path:\n",
    "    _ensure_cache(cfg)\n",
    "    out = cfg.macro_states_parquet\n",
    "    if out.exists() and not overwrite:\n",
    "        return out\n",
    "    states_lf.sink_parquet(str(out), compression=\"zstd\", statistics=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ee23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_macro_to_algo_features(cfg: MacroConfig, macro_states_path: Path, overwrite: bool = False) -> Path:\n",
    "    _ensure_cache(cfg)\n",
    "    if not cfg.algos_features_good.exists():\n",
    "        raise MacroError(f\"No existe {cfg.algos_features_good}. Ejecuta el preprocesado base primero.\")\n",
    "\n",
    "    out = cfg.algos_features_macro_parquet\n",
    "    if out.exists() and not overwrite:\n",
    "        return out\n",
    "\n",
    "    feats = pl.scan_parquet(str(cfg.algos_features_good))\n",
    "    macro = pl.scan_parquet(str(macro_states_path))\n",
    "\n",
    "    # Join por date (algos es diario; macro puede ser mensual -> si tu macro es mensual,\n",
    "    # conviene que macro.date sea el 1er día del mes y tú expandas a diario.\n",
    "    # Aquí hacemos forward-fill mensual->diario con un asof join.\n",
    "    # Polars: join_asof requiere sorted.\n",
    "    feats_s = feats.sort(\"date\")\n",
    "    macro_s = macro.sort(\"date\")\n",
    "\n",
    "    joined = (\n",
    "        feats_s.join_asof(\n",
    "            macro_s,\n",
    "            on=\"date\",\n",
    "            strategy=\"backward\"  # usa el último macro conocido en esa fecha\n",
    "        )\n",
    "        .filter(pl.col(\"regime_id\").is_not_null())\n",
    "    )\n",
    "\n",
    "    joined.sink_parquet(str(out), compression=\"zstd\", statistics=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23463f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_algo_regime_profiles(cfg: MacroConfig, features_macro_path: Path, overwrite: bool = False) -> Dict[str, Path]:\n",
    "    _ensure_cache(cfg)\n",
    "\n",
    "    out_long = cfg.regime_profiles_long\n",
    "    out_wide = cfg.regime_profiles_wide\n",
    "    out_rank = cfg.regime_rankings\n",
    "\n",
    "    if out_long.exists() and out_wide.exists() and out_rank.exists() and not overwrite:\n",
    "        return {\"long\": out_long, \"wide\": out_wide, \"rankings\": out_rank}\n",
    "\n",
    "    lf = pl.scan_parquet(str(features_macro_path))\n",
    "\n",
    "    # Métricas por (algo, régimen): mean, std, sharpe, max drawdown aproximado\n",
    "    # Nota: el drawdown aquí se calcula sobre \"close\" dentro del conjunto de fechas del régimen.\n",
    "    # Es una proxy útil para clustering; si quieres drawdown por segmentos contiguos, se puede refinar después.\n",
    "    annual = 252.0\n",
    "\n",
    "    prof = (\n",
    "        lf.group_by([\"algo_id\", \"regime_id\"])\n",
    "          .agg([\n",
    "              pl.len().alias(\"n_days\"),\n",
    "              pl.col(\"ret_1d\").mean().alias(\"ret_mean\"),\n",
    "              pl.col(\"ret_1d\").std().alias(\"ret_std\"),\n",
    "              (pl.col(\"close\") / pl.col(\"close\").cum_max() - 1.0).min().alias(\"max_drawdown\"),\n",
    "          ])\n",
    "          .with_columns([\n",
    "              (pl.col(\"ret_std\") * np.sqrt(annual)).alias(\"vol_ann\"),\n",
    "              pl.when(pl.col(\"ret_std\") > 0)\n",
    "                .then((pl.col(\"ret_mean\") / pl.col(\"ret_std\")) * np.sqrt(annual))\n",
    "                .otherwise(None)\n",
    "                .alias(\"sharpe_ann\"),\n",
    "          ])\n",
    "          .with_columns([\n",
    "              pl.when(pl.col(\"regime_id\") == 0).then(pl.lit(\"Reflation\"))\n",
    "               .when(pl.col(\"regime_id\") == 1).then(pl.lit(\"Recovery\"))\n",
    "               .when(pl.col(\"regime_id\") == 2).then(pl.lit(\"Overheat\"))\n",
    "               .otherwise(pl.lit(\"Stagflation\"))\n",
    "               .alias(\"regime_name\")\n",
    "          ])\n",
    "    )\n",
    "\n",
    "    prof.sink_parquet(str(out_long), compression=\"zstd\", statistics=True)\n",
    "\n",
    "    # Vector 4D por algoritmo (p.ej. Sharpe por régimen). Esto es lo que sugiere el PDF para clustering.\n",
    "    # pivot() requiere DataFrame (eager), no LazyFrame\n",
    "    wide_df = (\n",
    "        pl.read_parquet(str(out_long))\n",
    "        .select([\"algo_id\", \"regime_id\", \"sharpe_ann\", \"ret_mean\", \"max_drawdown\", \"n_days\"])\n",
    "        .pivot(\n",
    "            on=\"regime_id\",\n",
    "            index=\"algo_id\",\n",
    "            values=\"sharpe_ann\",\n",
    "            aggregate_function=\"first\",\n",
    "        )\n",
    "        .rename({\n",
    "            \"0\": \"sharpe_reflation\",\n",
    "            \"1\": \"sharpe_recovery\",\n",
    "            \"2\": \"sharpe_overheat\",\n",
    "            \"3\": \"sharpe_stagflation\",\n",
    "        })\n",
    "        .with_columns([\n",
    "            pl.all().exclude(\"algo_id\").fill_null(0.0)\n",
    "        ])\n",
    "    )\n",
    "    wide_df.write_parquet(str(out_wide), compression=\"zstd\", statistics=True)\n",
    "\n",
    "    # Rankings por régimen (para priors / action-masking contextual).\n",
    "    rankings = (\n",
    "        pl.scan_parquet(str(out_long))\n",
    "        .filter(pl.col(\"sharpe_ann\").is_not_null())\n",
    "        .with_columns([\n",
    "            pl.col(\"sharpe_ann\").rank(method=\"dense\", descending=True).over(\"regime_id\").alias(\"rank\"),\n",
    "        ])\n",
    "        .select([\"regime_id\", \"regime_name\", \"algo_id\", \"n_days\", \"sharpe_ann\", \"ret_mean\", \"vol_ann\", \"max_drawdown\", \"rank\"])\n",
    "        .sort([\"regime_id\", \"rank\"])\n",
    "    )\n",
    "    rankings.sink_parquet(str(out_rank), compression=\"zstd\", statistics=True)\n",
    "\n",
    "    return {\"long\": out_long, \"wide\": out_wide, \"rankings\": out_rank}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6b0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_numpy(X: np.ndarray, k: int, seed: int = 42, iters: int = 50) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    KMeans simple (kmeans++ init), devuelve (labels, centers).\n",
    "    X: (n, d)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n, d = X.shape\n",
    "    if k <= 1 or k > n:\n",
    "        raise ValueError(f\"k inválido: {k} para n={n}\")\n",
    "\n",
    "    # kmeans++ init\n",
    "    centers = np.empty((k, d), dtype=np.float64)\n",
    "    idx0 = rng.integers(0, n)\n",
    "    centers[0] = X[idx0]\n",
    "    dist2 = np.sum((X - centers[0]) ** 2, axis=1)\n",
    "\n",
    "    for i in range(1, k):\n",
    "        probs = dist2 / (dist2.sum() + 1e-12)\n",
    "        idx = rng.choice(n, p=probs)\n",
    "        centers[i] = X[idx]\n",
    "        new_dist2 = np.sum((X - centers[i]) ** 2, axis=1)\n",
    "        dist2 = np.minimum(dist2, new_dist2)\n",
    "\n",
    "    labels = np.zeros(n, dtype=np.int32)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        # assign\n",
    "        dists = np.sum((X[:, None, :] - centers[None, :, :]) ** 2, axis=2)  # (n,k)\n",
    "        new_labels = np.argmin(dists, axis=1).astype(np.int32)\n",
    "\n",
    "        if np.array_equal(new_labels, labels):\n",
    "            break\n",
    "        labels = new_labels\n",
    "\n",
    "        # update\n",
    "        for j in range(k):\n",
    "            mask = labels == j\n",
    "            if mask.any():\n",
    "                centers[j] = X[mask].mean(axis=0)\n",
    "            else:\n",
    "                # cluster vacío: re-seed\n",
    "                centers[j] = X[rng.integers(0, n)]\n",
    "\n",
    "    return labels, centers\n",
    "\n",
    "def cluster_algos_by_regime_profile(cfg: MacroConfig, profiles_wide_path: Path, k: int = 50, overwrite: bool = False) -> Path:\n",
    "    _ensure_cache(cfg)\n",
    "    out = cfg.clusters_parquet\n",
    "    if out.exists() and not overwrite:\n",
    "        return out\n",
    "\n",
    "    df = pl.read_parquet(str(profiles_wide_path))\n",
    "    if df.height == 0:\n",
    "        raise MacroError(\"profiles_wide está vacío.\")\n",
    "\n",
    "    algo_ids = df[\"algo_id\"].to_list()\n",
    "    X = df.select([\"sharpe_reflation\", \"sharpe_recovery\", \"sharpe_overheat\", \"sharpe_stagflation\"]).to_numpy()\n",
    "\n",
    "    # Normalización robusta (para clustering estable)\n",
    "    mu = np.nanmean(X, axis=0)\n",
    "    sd = np.nanstd(X, axis=0) + 1e-9\n",
    "    Xn = (X - mu) / sd\n",
    "\n",
    "    labels, centers = kmeans_numpy(Xn, k=k, seed=42, iters=60)\n",
    "\n",
    "    out_df = pl.DataFrame({\n",
    "        \"algo_id\": algo_ids,\n",
    "        \"cluster_id\": labels.astype(np.int32),\n",
    "    })\n",
    "\n",
    "    out_df.write_parquet(str(out), compression=\"zstd\", statistics=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51b478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_forensics_by_regime(cfg: MacroConfig, macro_states_path: Path, overwrite: bool = False) -> Path:\n",
    "    _ensure_cache(cfg)\n",
    "    out = cfg.cache_dir / \"benchmark_forensics_by_regime.parquet\"\n",
    "    if out.exists() and not overwrite:\n",
    "        return out\n",
    "\n",
    "    if not cfg.benchmark_trades_clean.exists():\n",
    "        raise MacroError(f\"No existe {cfg.benchmark_trades_clean}. Ejecuta benchmark preprocessing base primero.\")\n",
    "\n",
    "    trades = pl.scan_parquet(str(cfg.benchmark_trades_clean)).select([\n",
    "        pl.col(\"productname\").alias(\"algo_id\"),\n",
    "        \"volume\", \"open_date\", \"close_date\", \"holding_days\",\n",
    "        \"equity_EOD\", \"AUM\", \"equity_normalized\"\n",
    "    ]).filter(pl.col(\"open_date\").is_not_null())\n",
    "\n",
    "    macro = pl.scan_parquet(str(macro_states_path)).select([\"date\", \"regime_id\", \"regime_name\"]).sort(\"date\")\n",
    "\n",
    "    # asignamos a cada trade el régimen macro vigente en la fecha de apertura (backward asof)\n",
    "    trades_s = trades.sort(\"open_date\").rename({\"open_date\": \"date\"})\n",
    "    joined = trades_s.join_asof(macro, on=\"date\", strategy=\"backward\").filter(pl.col(\"regime_id\").is_not_null())\n",
    "\n",
    "    # stats por régimen\n",
    "    stats = (\n",
    "        joined.group_by([\"regime_id\", \"regime_name\"])\n",
    "              .agg([\n",
    "                  pl.len().alias(\"n_trades_open\"),\n",
    "                  pl.col(\"algo_id\").n_unique().alias(\"unique_algos_traded\"),\n",
    "                  pl.col(\"volume\").sum().alias(\"sum_volume\"),\n",
    "                  pl.col(\"volume\").mean().alias(\"avg_volume\"),\n",
    "                  pl.col(\"holding_days\").mean().alias(\"avg_holding_days\"),\n",
    "                  pl.col(\"equity_normalized\").mean().alias(\"avg_equity_norm\"),\n",
    "                  pl.col(\"AUM\").mean().alias(\"avg_AUM\"),\n",
    "              ])\n",
    "              .sort(\"regime_id\")\n",
    "    )\n",
    "\n",
    "    stats.sink_parquet(str(out), compression=\"zstd\", statistics=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef943fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_27268\\4263822301.py:15: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  cols = lf.columns\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_27268\\4263822301.py:72: DeprecationWarning: the argument `min_periods` for `Expr.rolling_median` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  infl_thr = pl.col(\"inflation_yoy\").rolling_median(W, min_periods=MINP).shift(1)\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_27268\\4263822301.py:73: DeprecationWarning: the argument `min_periods` for `Expr.rolling_median` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  grow_thr = pl.col(\"growth_yoy\").rolling_median(W, min_periods=MINP).shift(1)\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_27268\\4263822301.py:134: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  num_cols = [c for c in macro_lf.columns if c not in exclude]\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_27268\\4263822301.py:149: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  return out.select([c for c in keep if c in out.columns])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'macro_states': WindowsPath('cache/macro_states.parquet'),\n",
       " 'algos_features_macro': WindowsPath('cache/algos_features_macro.parquet'),\n",
       " 'algo_regime_profiles_long': WindowsPath('cache/algo_regime_profiles_long.parquet'),\n",
       " 'algo_regime_profiles_wide': WindowsPath('cache/algo_regime_profiles_wide.parquet'),\n",
       " 'algo_regime_rankings': WindowsPath('cache/algo_regime_rankings.parquet'),\n",
       " 'algo_regime_clusters': WindowsPath('cache/algo_regime_clusters.parquet'),\n",
       " 'benchmark_forensics_by_regime': WindowsPath('cache/benchmark_forensics_by_regime.parquet')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_macro_pipeline(cfg: MacroConfig, overwrite: bool = False, k_clusters: int = 50) -> Dict[str, Path]:\n",
    "    _ensure_cache(cfg)\n",
    "\n",
    "    macro_lf = load_macro(cfg)\n",
    "    states_lf = compute_investment_clock_states(cfg, macro_lf)\n",
    "    macro_states_path = write_macro_states(cfg, states_lf, overwrite=overwrite)\n",
    "\n",
    "    features_macro_path = join_macro_to_algo_features(cfg, macro_states_path, overwrite=overwrite)\n",
    "\n",
    "    prof_paths = compute_algo_regime_profiles(cfg, features_macro_path, overwrite=overwrite)\n",
    "    clusters_path = cluster_algos_by_regime_profile(cfg, prof_paths[\"wide\"], k=k_clusters, overwrite=overwrite)\n",
    "\n",
    "    bench_forensics_path = benchmark_forensics_by_regime(cfg, macro_states_path, overwrite=overwrite)\n",
    "\n",
    "    return {\n",
    "        \"macro_states\": macro_states_path,\n",
    "        \"algos_features_macro\": features_macro_path,\n",
    "        \"algo_regime_profiles_long\": prof_paths[\"long\"],\n",
    "        \"algo_regime_profiles_wide\": prof_paths[\"wide\"],\n",
    "        \"algo_regime_rankings\": prof_paths[\"rankings\"],\n",
    "        \"algo_regime_clusters\": clusters_path,\n",
    "        \"benchmark_forensics_by_regime\": bench_forensics_path,\n",
    "    }\n",
    "\n",
    "cfg = MacroConfig(\n",
    "    cache_dir=Path(\"data/cache\"),\n",
    "    algos_features_good=Path(\"data/cache/algos_features_good.parquet\"),\n",
    "    benchmark_trades_clean=Path(\"data/cache/benchmark_trades_clean.parquet\"),\n",
    "    macro_csv=Path(\"data/datos_competicion/macro_data.csv\"),  # <-- pon tu CSV aquí\n",
    "    regime_method=\"rolling_median\",\n",
    "    window_months=36,\n",
    "    min_months=12,\n",
    "    lag_months=0,     # si quieres evitar look-ahead por publicación, pon 1\n",
    ")\n",
    "\n",
    "paths = run_macro_pipeline(cfg, overwrite=False, k_clusters=50)\n",
    "paths\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
